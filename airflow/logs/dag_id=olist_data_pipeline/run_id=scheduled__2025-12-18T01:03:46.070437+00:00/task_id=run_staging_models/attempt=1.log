{"timestamp":"2025-12-18T01:03:51.741507Z","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-12-18T01:03:51.742135Z","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/olist_data_pipeline.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-12-18T01:03:51.772536Z","level":"info","event":"Task instance is in running state","logger":"task.stdout"}
{"timestamp":"2025-12-18T01:03:51.772688Z","level":"info","event":" Previous state of the Task instance: TaskInstanceState.QUEUED","logger":"task.stdout"}
{"timestamp":"2025-12-18T01:03:51.772855Z","level":"info","event":"Current task name:run_staging_models","logger":"task.stdout"}
{"timestamp":"2025-12-18T01:03:51.772978Z","level":"info","event":"Dag name:olist_data_pipeline","logger":"task.stdout"}
{"timestamp":"2025-12-18T01:03:51.773397Z","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":78}
{"timestamp":"2025-12-18T01:03:51.774571Z","level":"info","event":"Running command: ['/usr/bin/bash', '-c', 'cd /opt/airflow/dbt && dbt run --select staging --vars \\'{\"batch_hours\": 1}\\' --profiles-dir /opt/airflow/dbt']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":80}
{"timestamp":"2025-12-18T01:03:51.775311Z","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":92}
{"timestamp":"2025-12-18T01:03:53.741204Z","level":"info","event":"\u001b[0m01:03:53  Running with dbt=1.9.1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:03:56.136306Z","level":"info","event":"\u001b[0m01:03:56  Registered adapter: bigquery=1.9.1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:03:56.450547Z","level":"info","event":"\u001b[0m01:03:56  Unable to do partial parsing because config vars, config profile, or config target have changed","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:03:59.021866Z","level":"info","event":"\u001b[0m01:03:59  Found 15 models, 79 data tests, 9 sources, 611 macros","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:03:59.025943Z","level":"info","event":"\u001b[0m01:03:59","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:03:59.026363Z","level":"info","event":"\u001b[0m01:03:59  Concurrency: 4 threads (target='dev')","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:03:59.026779Z","level":"info","event":"\u001b[0m01:03:59","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:00.006887Z","level":"info","event":"\u001b[0m01:04:00  1 of 9 START sql table model staging.stg_customers ............................. [RUN]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:00.009103Z","level":"info","event":"\u001b[0m01:04:00  2 of 9 START sql table model staging.stg_geolocation ........................... [RUN]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:00.009836Z","level":"info","event":"\u001b[0m01:04:00  3 of 9 START sql table model staging.stg_order_items ........................... [RUN]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:00.011237Z","level":"info","event":"\u001b[0m01:04:00  4 of 9 START sql table model staging.stg_order_payments ........................ [RUN]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:00.671710Z","level":"info","event":"/home/airflow/.local/lib/python3.12/site-packages/dbt/adapters/bigquery/connections.py:570: FutureWarning: job_retry must be explicitly set to None if job_id is set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:00.671859Z","level":"info","event":"BigQuery cannot retry a failed job by using the exact","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:00.671926Z","level":"info","event":"same ID. Setting job_id without explicitly disabling","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:00.671967Z","level":"info","event":"job_retry will raise an error in the future. To avoid this","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:00.672006Z","level":"info","event":"warning, either use job_id_prefix instead (preferred) or","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:00.672040Z","level":"info","event":"set job_retry=None.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:00.672074Z","level":"info","event":"  query_job = client.query(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:02.938432Z","level":"info","event":"\u001b[0m01:04:02  4 of 9 OK created sql table model staging.stg_order_payments ................... [\u001b[32mCREATE TABLE (103.9k rows, 6.9 MiB processed)\u001b[0m in 2.92s]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:02.940370Z","level":"info","event":"\u001b[0m01:04:02  5 of 9 START sql table model staging.stg_order_reviews ......................... [RUN]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.173898Z","level":"info","event":"/home/airflow/.local/lib/python3.12/site-packages/dbt/adapters/bigquery/connections.py:570: FutureWarning: job_retry must be explicitly set to None if job_id is set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.174048Z","level":"info","event":"BigQuery cannot retry a failed job by using the exact","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.174097Z","level":"info","event":"same ID. Setting job_id without explicitly disabling","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.174135Z","level":"info","event":"job_retry will raise an error in the future. To avoid this","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.174170Z","level":"info","event":"warning, either use job_id_prefix instead (preferred) or","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.174204Z","level":"info","event":"set job_retry=None.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.174238Z","level":"info","event":"  query_job = client.query(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.609344Z","level":"info","event":"\u001b[0m01:04:03  3 of 9 OK created sql table model staging.stg_order_items ...................... [\u001b[32mCREATE TABLE (112.7k rows, 14.4 MiB processed)\u001b[0m in 3.60s]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.611178Z","level":"info","event":"\u001b[0m01:04:03  6 of 9 START sql incremental model staging.stg_orders .......................... [RUN]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.663136Z","level":"info","event":"/home/airflow/.local/lib/python3.12/site-packages/dbt/adapters/bigquery/connections.py:570: FutureWarning: job_retry must be explicitly set to None if job_id is set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.663295Z","level":"info","event":"BigQuery cannot retry a failed job by using the exact","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.663348Z","level":"info","event":"same ID. Setting job_id without explicitly disabling","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.663396Z","level":"info","event":"job_retry will raise an error in the future. To avoid this","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.663435Z","level":"info","event":"warning, either use job_id_prefix instead (preferred) or","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.663470Z","level":"info","event":"set job_retry=None.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.663505Z","level":"info","event":"  query_job = client.query(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.814883Z","level":"info","event":"\u001b[0m01:04:03  1 of 9 OK created sql table model staging.stg_customers ........................ [\u001b[32mCREATE TABLE (99.4k rows, 8.8 MiB processed)\u001b[0m in 3.80s]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:03.818010Z","level":"info","event":"\u001b[0m01:04:03  7 of 9 START sql table model staging.stg_product_category_translation .......... [RUN]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.061710Z","level":"info","event":"/home/airflow/.local/lib/python3.12/site-packages/dbt/adapters/bigquery/connections.py:570: FutureWarning: job_retry must be explicitly set to None if job_id is set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.061860Z","level":"info","event":"BigQuery cannot retry a failed job by using the exact","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.061924Z","level":"info","event":"same ID. Setting job_id without explicitly disabling","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.061963Z","level":"info","event":"job_retry will raise an error in the future. To avoid this","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.061999Z","level":"info","event":"warning, either use job_id_prefix instead (preferred) or","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.062032Z","level":"info","event":"set job_retry=None.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.062065Z","level":"info","event":"  query_job = client.query(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.375624Z","level":"info","event":"/home/airflow/.local/lib/python3.12/site-packages/dbt/adapters/bigquery/connections.py:570: FutureWarning: job_retry must be explicitly set to None if job_id is set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.375777Z","level":"info","event":"BigQuery cannot retry a failed job by using the exact","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.375825Z","level":"info","event":"same ID. Setting job_id without explicitly disabling","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.375864Z","level":"info","event":"job_retry will raise an error in the future. To avoid this","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.375917Z","level":"info","event":"warning, either use job_id_prefix instead (preferred) or","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.375952Z","level":"info","event":"set job_retry=None.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.375985Z","level":"info","event":"  query_job = client.query(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.771943Z","level":"info","event":"\u001b[0m01:04:04  2 of 9 OK created sql table model staging.stg_geolocation ...................... [\u001b[32mCREATE TABLE (1.0m rows, 38.7 MiB processed)\u001b[0m in 4.76s]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:04.775851Z","level":"info","event":"\u001b[0m01:04:04  8 of 9 START sql table model staging.stg_products .............................. [RUN]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.024741Z","level":"info","event":"/home/airflow/.local/lib/python3.12/site-packages/dbt/adapters/bigquery/connections.py:570: FutureWarning: job_retry must be explicitly set to None if job_id is set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.024958Z","level":"info","event":"BigQuery cannot retry a failed job by using the exact","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.025051Z","level":"info","event":"same ID. Setting job_id without explicitly disabling","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.025121Z","level":"info","event":"job_retry will raise an error in the future. To avoid this","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.025184Z","level":"info","event":"warning, either use job_id_prefix instead (preferred) or","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.025245Z","level":"info","event":"set job_retry=None.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.025304Z","level":"info","event":"  query_job = client.query(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.688630Z","level":"info","event":"\u001b[0m01:04:05  5 of 9 OK created sql table model staging.stg_order_reviews .................... [\u001b[32mCREATE TABLE (95.4k rows, 10.9 MiB processed)\u001b[0m in 2.75s]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.690767Z","level":"info","event":"\u001b[0m01:04:05  9 of 9 START sql table model staging.stg_sellers ............................... [RUN]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.954742Z","level":"info","event":"/home/airflow/.local/lib/python3.12/site-packages/dbt/adapters/bigquery/connections.py:570: FutureWarning: job_retry must be explicitly set to None if job_id is set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.954929Z","level":"info","event":"BigQuery cannot retry a failed job by using the exact","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.955020Z","level":"info","event":"same ID. Setting job_id without explicitly disabling","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.955087Z","level":"info","event":"job_retry will raise an error in the future. To avoid this","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.955135Z","level":"info","event":"warning, either use job_id_prefix instead (preferred) or","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.955172Z","level":"info","event":"set job_retry=None.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:05.955206Z","level":"info","event":"  query_job = client.query(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:06.274999Z","level":"info","event":"\u001b[0m01:04:06  7 of 9 OK created sql table model staging.stg_product_category_translation ..... [\u001b[32mCREATE TABLE (72.0 rows, 2.6 KiB processed)\u001b[0m in 2.45s]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:06.367105Z","level":"info","event":"/home/airflow/.local/lib/python3.12/site-packages/dbt/adapters/bigquery/connections.py:570: FutureWarning: job_retry must be explicitly set to None if job_id is set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:06.367295Z","level":"info","event":"BigQuery cannot retry a failed job by using the exact","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:06.367378Z","level":"info","event":"same ID. Setting job_id without explicitly disabling","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:06.367444Z","level":"info","event":"job_retry will raise an error in the future. To avoid this","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:06.367512Z","level":"info","event":"warning, either use job_id_prefix instead (preferred) or","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:06.367576Z","level":"info","event":"set job_retry=None.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:06.367638Z","level":"info","event":"  query_job = client.query(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:06.956423Z","level":"info","event":"\u001b[0m01:04:06  8 of 9 OK created sql table model staging.stg_products ......................... [\u001b[32mCREATE TABLE (33.0k rows, 3.3 MiB processed)\u001b[0m in 2.18s]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:08.590426Z","level":"info","event":"\u001b[0m01:04:08  6 of 9 OK created sql incremental model staging.stg_orders ..................... [\u001b[32mMERGE (0.0 rows, 12.0 MiB processed)\u001b[0m in 4.98s]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:16.499028Z","level":"info","event":"\u001b[0m01:04:16  9 of 9 OK created sql table model staging.stg_sellers .......................... [\u001b[32mCREATE TABLE (3.1k rows, 175.8 KiB processed)\u001b[0m in 10.81s]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:16.543147Z","level":"info","event":"\u001b[0m01:04:16","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:16.543492Z","level":"info","event":"\u001b[0m01:04:16  Finished running 1 incremental model, 8 table models in 0 hours 0 minutes and 17.52 seconds (17.52s).","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:16.759914Z","level":"info","event":"\u001b[0m01:04:16","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:16.760337Z","level":"info","event":"\u001b[0m01:04:16  \u001b[32mCompleted successfully\u001b[0m","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:16.760739Z","level":"info","event":"\u001b[0m01:04:16","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:16.761189Z","level":"info","event":"\u001b[0m01:04:16  Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":99}
{"timestamp":"2025-12-18T01:04:17.549118Z","level":"info","event":"Command exited with return code 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook","filename":"subprocess.py","lineno":103}
{"timestamp":"2025-12-18T01:04:17.549837Z","level":"info","event":"Pushing xcom","ti":"RuntimeTaskInstance(id=UUID('019b2efc-024b-7934-bbc7-e30bd74ebb77'), task_id='run_staging_models', dag_id='olist_data_pipeline', run_id='scheduled__2025-12-18T01:03:46.070437+00:00', try_number=1, dag_version_id=UUID('019b24bb-03ad-747b-b2de-db1e9d7a1132'), map_index=-1, hostname='94878978f23c', context_carrier={}, task=<Task(BashOperator): run_staging_models>, bundle_instance=LocalDagBundle(name=dags-folder), max_tries=2, start_date=datetime.datetime(2025, 12, 18, 1, 3, 51, 397578, tzinfo=datetime.timezone.utc), end_date=None, state=<TaskInstanceState.RUNNING: 'running'>, is_mapped=False, rendered_map_index=None)","logger":"task","filename":"task_runner.py","lineno":1370}
{"timestamp":"2025-12-18T01:04:17.579115Z","level":"info","event":"Task instance in success state","logger":"task.stdout"}
{"timestamp":"2025-12-18T01:04:17.579297Z","level":"info","event":" Previous state of the Task instance: TaskInstanceState.RUNNING","logger":"task.stdout"}
{"timestamp":"2025-12-18T01:04:17.579563Z","level":"info","event":"Task operator:<Task(BashOperator): run_staging_models>","logger":"task.stdout"}

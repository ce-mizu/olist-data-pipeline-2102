[2025-12-13T17:31:39.421+0000] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
[2025-12-13T17:31:39.462+0000] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2025-12-13T17:31:39.463+0000] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2025-12-13T17:31:39.469+0000] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 72792
[2025-12-13T17:31:39.471+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T17:31:39.478+0000] {settings.py:63} INFO - Configured default timezone America/Sao_Paulo
[2025-12-13T17:31:39.513+0000] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-12-13T17:36:39.534+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T17:38:41.958+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [scheduled]>
[2025-12-13T17:38:41.958+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T17:38:41.959+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [scheduled]>
[2025-12-13T17:38:41.962+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T17:38:41.962+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='run_staging_models', run_id='manual__2024-01-01T00:00:00-03:00', try_number=2, map_index=-1) to SequentialExecutor with priority 4 and queue default
[2025-12-13T17:38:41.962+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'run_staging_models', 'manual__2024-01-01T00:00:00-03:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:38:41.967+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'run_staging_models', 'manual__2024-01-01T00:00:00-03:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:38:43.472+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T17:38:43.674+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T17:38:44.273+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='run_staging_models', run_id='manual__2024-01-01T00:00:00-03:00', try_number=2, map_index=-1)
[2025-12-13T17:38:44.279+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=run_staging_models, run_id=manual__2024-01-01T00:00:00-03:00, map_index=-1, run_start_date=2025-12-13 17:38:43.717546+00:00, run_end_date=2025-12-13 17:38:44.136079+00:00, run_duration=0.418533, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=2, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-12-13 17:38:41.960091+00:00, queued_by_job_id=1, pid=79946
[2025-12-13T17:41:39.578+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T17:43:44.849+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [scheduled]>
[2025-12-13T17:43:44.849+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T17:43:44.850+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [scheduled]>
[2025-12-13T17:43:44.851+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T17:43:44.852+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='run_staging_models', run_id='manual__2024-01-01T00:00:00-03:00', try_number=3, map_index=-1) to SequentialExecutor with priority 4 and queue default
[2025-12-13T17:43:44.852+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'run_staging_models', 'manual__2024-01-01T00:00:00-03:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:43:44.857+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'run_staging_models', 'manual__2024-01-01T00:00:00-03:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:43:46.348+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T17:43:46.550+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T17:43:47.145+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='run_staging_models', run_id='manual__2024-01-01T00:00:00-03:00', try_number=3, map_index=-1)
[2025-12-13T17:43:47.149+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=run_staging_models, run_id=manual__2024-01-01T00:00:00-03:00, map_index=-1, run_start_date=2025-12-13 17:43:46.598374+00:00, run_end_date=2025-12-13 17:43:46.756596+00:00, run_duration=0.158222, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-12-13 17:43:44.850957+00:00, queued_by_job_id=1, pid=80911
[2025-12-13T17:43:47.810+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
[2025-12-13T17:43:47.811+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T17:43:47.811+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
[2025-12-13T17:43:47.812+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T17:43:47.813+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T17:43:47.813+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:43:47.817+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:43:49.358+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T17:43:49.559+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T17:43:50.137+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=1, map_index=-1)
[2025-12-13T17:43:50.141+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:38:12.108287+00:00, map_index=-1, run_start_date=2025-12-13 17:43:49.598210+00:00, run_end_date=2025-12-13 17:43:49.751262+00:00, run_duration=0.153052, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:43:47.812215+00:00, queued_by_job_id=1, pid=81112
[2025-12-13T17:46:39.620+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T17:48:49.990+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
[2025-12-13T17:48:49.990+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T17:48:49.990+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
[2025-12-13T17:48:49.992+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T17:48:49.992+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T17:48:49.992+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:48:49.996+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:48:51.541+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T17:48:51.745+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T17:48:52.333+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=2, map_index=-1)
[2025-12-13T17:48:52.337+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:38:12.108287+00:00, map_index=-1, run_start_date=2025-12-13 17:48:51.786508+00:00, run_end_date=2025-12-13 17:48:51.934865+00:00, run_duration=0.148357, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=5, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:48:49.991405+00:00, queued_by_job_id=1, pid=83268
[2025-12-13T17:51:39.664+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T17:53:52.858+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
[2025-12-13T17:53:52.858+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T17:53:52.859+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
[2025-12-13T17:53:52.860+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T17:53:52.860+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T17:53:52.861+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:53:52.865+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:53:54.364+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T17:53:54.569+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T17:53:55.179+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=3, map_index=-1)
[2025-12-13T17:53:55.183+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:38:12.108287+00:00, map_index=-1, run_start_date=2025-12-13 17:53:54.614212+00:00, run_end_date=2025-12-13 17:53:54.782413+00:00, run_duration=0.168201, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=2, job_id=6, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:53:52.859707+00:00, queued_by_job_id=1, pid=83309
[2025-12-13T17:53:58.101+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 17:38:12.108287+00:00: manual__2025-12-13T17:38:12.108287+00:00, state:running, queued_at: 2025-12-13 17:38:12.124051+00:00. externally triggered: True> failed
[2025-12-13T17:53:58.102+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 17:38:12.108287+00:00, run_id=manual__2025-12-13T17:38:12.108287+00:00, run_start_date=2025-12-13 17:43:47.788171+00:00, run_end_date=2025-12-13 17:53:58.102413+00:00, run_duration=610.314242, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 16:38:12.108287+00:00, data_interval_end=2025-12-13 17:38:12.108287+00:00, dag_hash=ddcd72763e6860eb1041df6c52065120
[2025-12-13T17:53:59.165+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [scheduled]>
[2025-12-13T17:53:59.165+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T17:53:59.165+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [scheduled]>
[2025-12-13T17:53:59.167+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T17:53:59.167+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:40:15.725110+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T17:53:59.168+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:40:15.725110+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:53:59.171+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:40:15.725110+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:54:00.669+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T17:54:00.868+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T17:54:01.462+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:40:15.725110+00:00', try_number=1, map_index=-1)
[2025-12-13T17:54:01.466+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:40:15.725110+00:00, map_index=-1, run_start_date=2025-12-13 17:54:00.908124+00:00, run_end_date=2025-12-13 17:54:01.063873+00:00, run_duration=0.155749, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=7, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:53:59.166432+00:00, queued_by_job_id=1, pid=83315
[2025-12-13T17:54:48.491+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [scheduled]>
[2025-12-13T17:54:48.491+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T17:54:48.491+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [scheduled]>
[2025-12-13T17:54:48.493+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T17:54:48.493+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:40:15.725110+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T17:54:48.493+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:40:15.725110+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:54:48.498+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:40:15.725110+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:54:50.031+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T17:54:50.236+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T17:54:50.869+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:40:15.725110+00:00', try_number=2, map_index=-1)
[2025-12-13T17:54:50.873+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:40:15.725110+00:00, map_index=-1, run_start_date=2025-12-13 17:54:50.276180+00:00, run_end_date=2025-12-13 17:54:50.429163+00:00, run_duration=0.152983, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=3, job_id=8, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:54:48.492369+00:00, queued_by_job_id=1, pid=83333
[2025-12-13T17:55:19.257+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 17:40:15.725110+00:00: manual__2025-12-13T17:40:15.725110+00:00, state:running, queued_at: 2025-12-13 17:40:15.738680+00:00. externally triggered: True> failed
[2025-12-13T17:55:19.258+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 17:40:15.725110+00:00, run_id=manual__2025-12-13T17:40:15.725110+00:00, run_start_date=2025-12-13 17:53:59.141341+00:00, run_end_date=2025-12-13 17:55:19.258348+00:00, run_duration=80.117007, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 16:40:15.725110+00:00, data_interval_end=2025-12-13 17:40:15.725110+00:00, dag_hash=ddcd72763e6860eb1041df6c52065120
[2025-12-13T17:55:20.313+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
[2025-12-13T17:55:20.313+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T17:55:20.314+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
[2025-12-13T17:55:20.315+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T17:55:20.316+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=4, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T17:55:20.316+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:55:20.320+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:55:21.811+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T17:55:22.013+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T17:55:22.621+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=4, map_index=-1)
[2025-12-13T17:55:22.625+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:38:12.108287+00:00, map_index=-1, run_start_date=2025-12-13 17:55:22.054107+00:00, run_end_date=2025-12-13 17:55:22.208246+00:00, run_duration=0.154139, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=4, max_tries=5, job_id=9, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:55:20.314878+00:00, queued_by_job_id=1, pid=83345
[2025-12-13T17:55:53.946+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 17:38:12.108287+00:00: manual__2025-12-13T17:38:12.108287+00:00, state:running, queued_at: 2025-12-13 17:54:09.073687+00:00. externally triggered: True> failed
[2025-12-13T17:55:53.947+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 17:38:12.108287+00:00, run_id=manual__2025-12-13T17:38:12.108287+00:00, run_start_date=2025-12-13 17:55:20.290933+00:00, run_end_date=2025-12-13 17:55:53.947256+00:00, run_duration=33.656323, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 16:38:12.108287+00:00, data_interval_end=2025-12-13 17:38:12.108287+00:00, dag_hash=ddcd72763e6860eb1041df6c52065120
[2025-12-13T17:55:55.001+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:53:33.631212+00:00 [scheduled]>
[2025-12-13T17:55:55.001+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T17:55:55.002+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:53:33.631212+00:00 [scheduled]>
[2025-12-13T17:55:55.003+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:53:33.631212+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T17:55:55.003+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:53:33.631212+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T17:55:55.004+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:53:33.631212+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:55:55.008+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:53:33.631212+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T17:55:56.522+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T17:55:56.726+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:53:33.631212+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T17:55:57.323+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:53:33.631212+00:00', try_number=1, map_index=-1)
[2025-12-13T17:55:57.327+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:53:33.631212+00:00, map_index=-1, run_start_date=2025-12-13 17:55:56.766156+00:00, run_end_date=2025-12-13 17:55:56.920243+00:00, run_duration=0.154087, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=10, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:55:55.002738+00:00, queued_by_job_id=1, pid=83352
[2025-12-13T17:56:39.706+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T17:57:08.602+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 17:53:33.631212+00:00: manual__2025-12-13T17:53:33.631212+00:00, state:running, queued_at: 2025-12-13 17:53:33.649016+00:00. externally triggered: True> failed
[2025-12-13T17:57:08.602+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 17:53:33.631212+00:00, run_id=manual__2025-12-13T17:53:33.631212+00:00, run_start_date=2025-12-13 17:55:54.979762+00:00, run_end_date=2025-12-13 17:57:08.602846+00:00, run_duration=73.623084, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 16:53:33.631212+00:00, data_interval_end=2025-12-13 17:53:33.631212+00:00, dag_hash=1a042fa87aa613e8093b4a8ac4bedfd9
[2025-12-13T18:01:39.723+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T18:01:43.782+0000] {scheduler_job_runner.py:1526} INFO - DAG olist_data_pipeline is at (or above) max_active_runs (1 of 1), not creating any more runs
[2025-12-13T18:01:43.813+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps scheduled__2025-12-13T17:00:00+00:00 [scheduled]>
[2025-12-13T18:01:43.813+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:01:43.814+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps scheduled__2025-12-13T17:00:00+00:00 [scheduled]>
[2025-12-13T18:01:43.815+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps scheduled__2025-12-13T17:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:01:43.815+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='scheduled__2025-12-13T17:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:01:43.816+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'scheduled__2025-12-13T17:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:01:43.819+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'scheduled__2025-12-13T17:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:01:45.349+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:01:45.552+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps scheduled__2025-12-13T17:00:00+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:01:46.140+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='scheduled__2025-12-13T17:00:00+00:00', try_number=1, map_index=-1)
[2025-12-13T18:01:46.144+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=scheduled__2025-12-13T17:00:00+00:00, map_index=-1, run_start_date=2025-12-13 18:01:45.592745+00:00, run_end_date=2025-12-13 18:01:45.750170+00:00, run_duration=0.157425, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=11, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:01:43.814705+00:00, queued_by_job_id=1, pid=86666
[2025-12-13T18:06:18.268+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 17:00:00+00:00: scheduled__2025-12-13T17:00:00+00:00, state:running, queued_at: 2025-12-13 18:01:43.777878+00:00. externally triggered: False> failed
[2025-12-13T18:06:18.269+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 17:00:00+00:00, run_id=scheduled__2025-12-13T17:00:00+00:00, run_start_date=2025-12-13 18:01:43.792381+00:00, run_end_date=2025-12-13 18:06:18.269366+00:00, run_duration=274.476985, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-12-13 17:00:00+00:00, data_interval_end=2025-12-13 18:00:00+00:00, dag_hash=507b8780e869bd468d2cca259e6e880b
[2025-12-13T18:06:18.271+0000] {scheduler_job_runner.py:1526} INFO - DAG olist_data_pipeline is at (or above) max_active_runs (1 of 1), not creating any more runs
[2025-12-13T18:06:18.760+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:05:41.863370+00:00 [scheduled]>
[2025-12-13T18:06:18.760+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:06:18.761+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:05:41.863370+00:00 [scheduled]>
[2025-12-13T18:06:18.762+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:05:41.863370+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:06:18.762+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:05:41.863370+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:06:18.763+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:05:41.863370+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:06:18.766+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:05:41.863370+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:06:20.277+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:06:20.481+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:05:41.863370+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:06:21.082+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:05:41.863370+00:00', try_number=1, map_index=-1)
[2025-12-13T18:06:21.086+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:05:41.863370+00:00, map_index=-1, run_start_date=2025-12-13 18:06:20.521853+00:00, run_end_date=2025-12-13 18:06:20.676612+00:00, run_duration=0.154759, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=12, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:06:18.761649+00:00, queued_by_job_id=1, pid=88264
[2025-12-13T18:06:39.765+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T18:08:27.388+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:05:41.863370+00:00: manual__2025-12-13T18:05:41.863370+00:00, state:running, queued_at: 2025-12-13 18:05:41.883385+00:00. externally triggered: True> failed
[2025-12-13T18:08:27.388+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:05:41.863370+00:00, run_id=manual__2025-12-13T18:05:41.863370+00:00, run_start_date=2025-12-13 18:06:18.738855+00:00, run_end_date=2025-12-13 18:08:27.388714+00:00, run_duration=128.649859, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:05:41.863370+00:00, data_interval_end=2025-12-13 18:05:41.863370+00:00, dag_hash=a7520b485e48cae81884a168c46a1b46
[2025-12-13T18:08:40.889+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [scheduled]>
[2025-12-13T18:08:40.890+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:08:40.890+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [scheduled]>
[2025-12-13T18:08:40.892+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:08:40.892+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:08:39.403072+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:08:40.892+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:08:39.403072+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:08:40.896+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:08:39.403072+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:08:42.430+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:08:42.635+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:08:43.255+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:08:39.403072+00:00', try_number=1, map_index=-1)
[2025-12-13T18:08:43.259+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:08:39.403072+00:00, map_index=-1, run_start_date=2025-12-13 18:08:42.677104+00:00, run_end_date=2025-12-13 18:08:42.834411+00:00, run_duration=0.157307, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=13, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:08:40.891323+00:00, queued_by_job_id=1, pid=88795
[2025-12-13T18:11:39.809+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T18:13:43.191+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [scheduled]>
[2025-12-13T18:13:43.191+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:13:43.192+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [scheduled]>
[2025-12-13T18:13:43.193+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:13:43.194+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:08:39.403072+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:13:43.194+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:08:39.403072+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:13:43.198+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:08:39.403072+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:13:44.718+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:13:44.924+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:13:45.539+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:08:39.403072+00:00', try_number=2, map_index=-1)
[2025-12-13T18:13:45.543+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:08:39.403072+00:00, map_index=-1, run_start_date=2025-12-13 18:13:44.965791+00:00, run_end_date=2025-12-13 18:13:45.134607+00:00, run_duration=0.168816, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=14, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:13:43.192818+00:00, queued_by_job_id=1, pid=90908
[2025-12-13T18:16:39.853+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T18:16:45.655+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:08:39.403072+00:00: manual__2025-12-13T18:08:39.403072+00:00, state:running, queued_at: 2025-12-13 18:08:39.413900+00:00. externally triggered: True> failed
[2025-12-13T18:16:45.656+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:08:39.403072+00:00, run_id=manual__2025-12-13T18:08:39.403072+00:00, run_start_date=2025-12-13 18:08:40.868631+00:00, run_end_date=2025-12-13 18:16:45.656343+00:00, run_duration=484.787712, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:08:39.403072+00:00, data_interval_end=2025-12-13 18:08:39.403072+00:00, dag_hash=b84b530224233739795dbca4f33b9e9e
[2025-12-13T18:16:50.963+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:16:49.854927+00:00 [scheduled]>
[2025-12-13T18:16:50.963+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:16:50.964+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:16:49.854927+00:00 [scheduled]>
[2025-12-13T18:16:50.965+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:16:49.854927+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:16:50.966+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:16:49.854927+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:16:50.966+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:16:49.854927+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:16:50.970+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:16:49.854927+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:16:52.458+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:16:52.659+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:16:49.854927+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:16:53.270+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:16:49.854927+00:00', try_number=1, map_index=-1)
[2025-12-13T18:16:53.274+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:16:49.854927+00:00, map_index=-1, run_start_date=2025-12-13 18:16:52.699970+00:00, run_end_date=2025-12-13 18:16:52.858699+00:00, run_duration=0.158729, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=15, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:16:50.964760+00:00, queued_by_job_id=1, pid=94014
[2025-12-13T18:17:27.776+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:16:49.854927+00:00: manual__2025-12-13T18:16:49.854927+00:00, state:running, queued_at: 2025-12-13 18:16:49.866945+00:00. externally triggered: True> failed
[2025-12-13T18:17:27.776+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:16:49.854927+00:00, run_id=manual__2025-12-13T18:16:49.854927+00:00, run_start_date=2025-12-13 18:16:50.941287+00:00, run_end_date=2025-12-13 18:17:27.776750+00:00, run_duration=36.835463, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:16:49.854927+00:00, data_interval_end=2025-12-13 18:16:49.854927+00:00, dag_hash=b84b530224233739795dbca4f33b9e9e
[2025-12-13T18:17:30.302+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:17:29.776108+00:00 [scheduled]>
[2025-12-13T18:17:30.303+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:17:30.303+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:17:29.776108+00:00 [scheduled]>
[2025-12-13T18:17:30.304+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:17:29.776108+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:17:30.305+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:17:29.776108+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:17:30.305+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:17:29.776108+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:17:30.309+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:17:29.776108+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:17:31.788+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:17:31.987+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:17:29.776108+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:17:32.560+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:17:29.776108+00:00', try_number=1, map_index=-1)
[2025-12-13T18:17:32.565+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:17:29.776108+00:00, map_index=-1, run_start_date=2025-12-13 18:17:32.025191+00:00, run_end_date=2025-12-13 18:17:32.178140+00:00, run_duration=0.152949, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=16, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:17:30.303943+00:00, queued_by_job_id=1, pid=94266
[2025-12-13T18:21:34.705+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:17:29.776108+00:00: manual__2025-12-13T18:17:29.776108+00:00, state:running, queued_at: 2025-12-13 18:17:29.787331+00:00. externally triggered: True> failed
[2025-12-13T18:21:34.705+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:17:29.776108+00:00, run_id=manual__2025-12-13T18:17:29.776108+00:00, run_start_date=2025-12-13 18:17:30.280825+00:00, run_end_date=2025-12-13 18:21:34.705915+00:00, run_duration=244.42509, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:17:29.776108+00:00, data_interval_end=2025-12-13 18:17:29.776108+00:00, dag_hash=85d84bc0e65e1da28f644ab6f93d63f9
[2025-12-13T18:21:39.885+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T18:22:06.863+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:22:05.429871+00:00 [scheduled]>
[2025-12-13T18:22:06.864+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:22:06.864+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:22:05.429871+00:00 [scheduled]>
[2025-12-13T18:22:06.865+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:22:05.429871+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:22:06.866+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:22:05.429871+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:22:06.866+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:22:05.429871+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:22:06.870+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:22:05.429871+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:22:08.361+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:22:08.561+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:22:05.429871+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:22:09.145+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:22:05.429871+00:00', try_number=1, map_index=-1)
[2025-12-13T18:22:09.149+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:22:05.429871+00:00, map_index=-1, run_start_date=2025-12-13 18:22:08.599748+00:00, run_end_date=2025-12-13 18:22:08.751076+00:00, run_duration=0.151328, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=17, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:22:06.865106+00:00, queued_by_job_id=1, pid=95090
[2025-12-13T18:23:59.750+0000] {manager.py:537} INFO - DAG olist_data_pipeline is missing and will be deactivated.
[2025-12-13T18:23:59.755+0000] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-12-13T18:23:59.760+0000] {manager.py:553} INFO - Deleted DAG olist_data_pipeline in serialized_dag table
[2025-12-13T18:26:22.883+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:22:05.429871+00:00: manual__2025-12-13T18:22:05.429871+00:00, state:running, queued_at: 2025-12-13 18:22:05.443355+00:00. externally triggered: True> failed
[2025-12-13T18:26:22.883+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:22:05.429871+00:00, run_id=manual__2025-12-13T18:22:05.429871+00:00, run_start_date=2025-12-13 18:22:06.841950+00:00, run_end_date=2025-12-13 18:26:22.883543+00:00, run_duration=256.041593, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:22:05.429871+00:00, data_interval_end=2025-12-13 18:22:05.429871+00:00, dag_hash=7fd8b63a2bc08b52e367d2be1c072786
[2025-12-13T18:26:23.936+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:26:22.441273+00:00 [scheduled]>
[2025-12-13T18:26:23.937+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:26:23.937+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:26:22.441273+00:00 [scheduled]>
[2025-12-13T18:26:23.938+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:26:22.441273+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:26:23.939+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:26:22.441273+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:26:23.939+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:26:22.441273+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:26:23.943+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:26:22.441273+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:26:25.518+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:26:25.719+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:26:22.441273+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:26:26.317+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:26:22.441273+00:00', try_number=1, map_index=-1)
[2025-12-13T18:26:26.321+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:26:22.441273+00:00, map_index=-1, run_start_date=2025-12-13 18:26:25.759540+00:00, run_end_date=2025-12-13 18:26:25.920407+00:00, run_duration=0.160867, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=18, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:26:23.938087+00:00, queued_by_job_id=1, pid=99272
[2025-12-13T18:26:39.929+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T18:28:41.325+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:26:22.441273+00:00: manual__2025-12-13T18:26:22.441273+00:00, state:running, queued_at: 2025-12-13 18:26:22.453066+00:00. externally triggered: True> failed
[2025-12-13T18:28:41.325+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:26:22.441273+00:00, run_id=manual__2025-12-13T18:26:22.441273+00:00, run_start_date=2025-12-13 18:26:23.915648+00:00, run_end_date=2025-12-13 18:28:41.325885+00:00, run_duration=137.410237, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:26:22.441273+00:00, data_interval_end=2025-12-13 18:26:22.441273+00:00, dag_hash=7fd8b63a2bc08b52e367d2be1c072786
[2025-12-13T18:28:48.302+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:28:47.475210+00:00 [scheduled]>
[2025-12-13T18:28:48.303+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:28:48.303+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:28:47.475210+00:00 [scheduled]>
[2025-12-13T18:28:48.304+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:28:47.475210+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:28:48.305+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:28:47.475210+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:28:48.305+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:28:47.475210+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:28:48.309+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:28:47.475210+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:28:49.806+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:28:50.009+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:28:47.475210+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:28:50.591+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:28:47.475210+00:00', try_number=1, map_index=-1)
[2025-12-13T18:28:50.595+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:28:47.475210+00:00, map_index=-1, run_start_date=2025-12-13 18:28:50.050049+00:00, run_end_date=2025-12-13 18:28:50.209458+00:00, run_duration=0.159409, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=19, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:28:48.303917+00:00, queued_by_job_id=1, pid=99896
[2025-12-13T18:29:48.519+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:28:47.475210+00:00: manual__2025-12-13T18:28:47.475210+00:00, state:running, queued_at: 2025-12-13 18:28:47.482929+00:00. externally triggered: True> failed
[2025-12-13T18:29:48.520+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:28:47.475210+00:00, run_id=manual__2025-12-13T18:28:47.475210+00:00, run_start_date=2025-12-13 18:28:48.281282+00:00, run_end_date=2025-12-13 18:29:48.519982+00:00, run_duration=60.2387, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:28:47.475210+00:00, data_interval_end=2025-12-13 18:28:47.475210+00:00, dag_hash=7fd8b63a2bc08b52e367d2be1c072786
[2025-12-13T18:31:39.961+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T18:33:50.318+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [scheduled]>
[2025-12-13T18:33:50.319+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:33:50.319+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [scheduled]>
[2025-12-13T18:33:50.320+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:33:50.321+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:33:49.127472+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:33:50.321+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:33:49.127472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:33:50.325+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:33:49.127472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:33:51.817+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:33:52.025+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:33:52.614+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:33:49.127472+00:00', try_number=1, map_index=-1)
[2025-12-13T18:33:52.618+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:33:49.127472+00:00, map_index=-1, run_start_date=2025-12-13 18:33:52.066447+00:00, run_end_date=2025-12-13 18:33:52.221421+00:00, run_duration=0.154974, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=20, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:33:50.319901+00:00, queued_by_job_id=1, pid=101570
[2025-12-13T18:36:40.003+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T18:38:53.116+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [scheduled]>
[2025-12-13T18:38:53.117+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:38:53.117+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [scheduled]>
[2025-12-13T18:38:53.119+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:38:53.119+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:33:49.127472+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:38:53.120+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:33:49.127472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:38:53.124+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:33:49.127472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:38:54.627+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:38:54.828+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:38:55.427+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:33:49.127472+00:00', try_number=2, map_index=-1)
[2025-12-13T18:38:55.431+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:33:49.127472+00:00, map_index=-1, run_start_date=2025-12-13 18:38:54.868050+00:00, run_end_date=2025-12-13 18:38:55.018490+00:00, run_duration=0.15044, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=21, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:38:53.118257+00:00, queued_by_job_id=1, pid=103443
[2025-12-13T18:39:42.640+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:33:49.127472+00:00: manual__2025-12-13T18:33:49.127472+00:00, state:running, queued_at: 2025-12-13 18:33:49.138998+00:00. externally triggered: True> failed
[2025-12-13T18:39:42.641+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:33:49.127472+00:00, run_id=manual__2025-12-13T18:33:49.127472+00:00, run_start_date=2025-12-13 18:33:50.297910+00:00, run_end_date=2025-12-13 18:39:42.641488+00:00, run_duration=352.343578, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:33:49.127472+00:00, data_interval_end=2025-12-13 18:33:49.127472+00:00, dag_hash=c0bd5b3bcf52c5ee975bb131e83a1b14
[2025-12-13T18:39:45.604+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:39:44.313347+00:00 [scheduled]>
[2025-12-13T18:39:45.604+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:39:45.605+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:39:44.313347+00:00 [scheduled]>
[2025-12-13T18:39:45.606+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:39:44.313347+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:39:45.606+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:39:44.313347+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:39:45.607+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:39:44.313347+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:39:45.611+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:39:44.313347+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:39:47.122+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:39:47.323+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:39:44.313347+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:39:47.915+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:39:44.313347+00:00', try_number=1, map_index=-1)
[2025-12-13T18:39:47.919+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:39:44.313347+00:00, map_index=-1, run_start_date=2025-12-13 18:39:47.364801+00:00, run_end_date=2025-12-13 18:39:47.522121+00:00, run_duration=0.15732, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=22, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:39:45.605699+00:00, queued_by_job_id=1, pid=103683
[2025-12-13T18:41:40.047+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T18:41:56.951+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:39:44.313347+00:00: manual__2025-12-13T18:39:44.313347+00:00, state:running, queued_at: 2025-12-13 18:39:44.324888+00:00. externally triggered: True> failed
[2025-12-13T18:41:56.952+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:39:44.313347+00:00, run_id=manual__2025-12-13T18:39:44.313347+00:00, run_start_date=2025-12-13 18:39:45.581550+00:00, run_end_date=2025-12-13 18:41:56.952163+00:00, run_duration=131.370613, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:39:44.313347+00:00, data_interval_end=2025-12-13 18:39:44.313347+00:00, dag_hash=133c01d83d42e0b19420ba1a11a243ae
[2025-12-13T18:43:22.204+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:43:21.275679+00:00 [scheduled]>
[2025-12-13T18:43:22.204+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:43:22.205+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:43:21.275679+00:00 [scheduled]>
[2025-12-13T18:43:22.206+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:43:21.275679+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:43:22.206+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:43:21.275679+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:43:22.207+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:43:21.275679+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:43:22.211+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:43:21.275679+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:43:23.714+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:43:23.922+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:43:21.275679+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:43:24.507+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:43:21.275679+00:00', try_number=1, map_index=-1)
[2025-12-13T18:43:24.511+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:43:21.275679+00:00, map_index=-1, run_start_date=2025-12-13 18:43:23.961013+00:00, run_end_date=2025-12-13 18:43:24.119234+00:00, run_duration=0.158221, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=23, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:43:22.205685+00:00, queued_by_job_id=1, pid=105261
[2025-12-13T18:45:20.953+0000] {dagrun.py:823} ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:43:21.275679+00:00: manual__2025-12-13T18:43:21.275679+00:00, state:running, queued_at: 2025-12-13 18:43:21.283419+00:00. externally triggered: True> failed
[2025-12-13T18:45:20.954+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:43:21.275679+00:00, run_id=manual__2025-12-13T18:43:21.275679+00:00, run_start_date=2025-12-13 18:43:22.182397+00:00, run_end_date=2025-12-13 18:45:20.954265+00:00, run_duration=118.771868, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:43:21.275679+00:00, data_interval_end=2025-12-13 18:43:21.275679+00:00, dag_hash=2fce28ae88e791965b4f4102d100be12
[2025-12-13T18:45:26.450+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [scheduled]>
[2025-12-13T18:45:26.451+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:45:26.451+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [scheduled]>
[2025-12-13T18:45:26.452+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:45:26.453+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:45:25.229472+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:45:26.453+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:45:25.229472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:45:26.457+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:45:25.229472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:45:27.972+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:45:28.182+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:45:28.815+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:45:25.229472+00:00', try_number=1, map_index=-1)
[2025-12-13T18:45:28.819+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:45:25.229472+00:00, map_index=-1, run_start_date=2025-12-13 18:45:28.224853+00:00, run_end_date=2025-12-13 18:45:28.384720+00:00, run_duration=0.159867, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=24, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:45:26.451944+00:00, queued_by_job_id=1, pid=105935
[2025-12-13T18:46:40.089+0000] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-12-13T18:50:28.716+0000] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [scheduled]>
[2025-12-13T18:50:28.716+0000] {scheduler_job_runner.py:507} INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
[2025-12-13T18:50:28.717+0000] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [scheduled]>
[2025-12-13T18:50:28.718+0000] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-12-13T18:50:28.719+0000] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:45:25.229472+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2025-12-13T18:50:28.719+0000] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:45:25.229472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:50:28.723+0000] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:45:25.229472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
[2025-12-13T18:50:30.243+0000] {dagbag.py:588} INFO - Filling up the DagBag from /home/eduardomizumoto/code/ce-mizu/olist-data-pipeline-2102/airflow/dags/olist_data_pipeline.py
[2025-12-13T18:50:30.452+0000] {task_command.py:467} INFO - Running <TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [queued]> on host olist-data-pipeline-2102.us-central1-a.c.olist-data-pipeline.internal
[2025-12-13T18:50:31.036+0000] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:45:25.229472+00:00', try_number=2, map_index=-1)
[2025-12-13T18:50:31.040+0000] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:45:25.229472+00:00, map_index=-1, run_start_date=2025-12-13 18:50:30.492535+00:00, run_end_date=2025-12-13 18:50:30.644272+00:00, run_duration=0.151737, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=25, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:50:28.717760+00:00, queued_by_job_id=1, pid=106800

# Airflow para Olist Data Pipeline

Este diretÃ³rio contÃ©m a configuraÃ§Ã£o do Apache Airflow para orquestrar o pipeline de dados do Olist.

## ğŸ—ï¸ Estrutura

```
airflow/
â”œâ”€â”€ dags/                           # DAGs do Airflow
â”‚   â”œâ”€â”€ olist_data_pipeline.py     # Pipeline bÃ¡sico
â”‚   â””â”€â”€ olist_pipeline_advanced.py # Pipeline avanÃ§ado
â”œâ”€â”€ logs/                          # Logs do Airflow
â”œâ”€â”€ plugins/                       # Plugins customizados
â”œâ”€â”€ airflow.cfg                   # ConfiguraÃ§Ã£o do Airflow
â”œâ”€â”€ docker-compose.yml            # Setup com Docker
â”œâ”€â”€ Dockerfile                    # Imagem customizada
â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â””â”€â”€ init_airflow.sh              # Script de inicializaÃ§Ã£o
```

## ğŸš€ ConfiguraÃ§Ã£o RÃ¡pida

### OpÃ§Ã£o 1: Setup Local

```bash
# 1. Executar script de inicializaÃ§Ã£o
cd airflow
./init_airflow.sh

# 2. Iniciar serviÃ§os
airflow webserver --port 8080 &
airflow scheduler &
```

### OpÃ§Ã£o 2: Setup com Docker

```bash
# 1. Construir e iniciar containers
cd airflow
docker-compose up -d

# 2. Verificar status
docker-compose ps
```

## ğŸ“Š DAGs DisponÃ­veis

### 1. `olist_data_pipeline` (BÃ¡sico)
- **FrequÃªncia**: A cada hora
- **Funcionalidades**:
  - Executa modelos staging com batch de 1 hora
  - Processa modelos intermediate
  - Executa testes de qualidade
  - Gera documentaÃ§Ã£o

### 2. `olist_pipeline_advanced` (AvanÃ§ado)
- **FrequÃªncia**: A cada hora
- **Funcionalidades**:
  - Batch dinÃ¢mico baseado no horÃ¡rio
  - Controle granular por modelo
  - ValidaÃ§Ã£o por etapas
  - ParalelizaÃ§Ã£o inteligente

## âš™ï¸ ConfiguraÃ§Ãµes

### VariÃ¡veis do Airflow
```python
# Batch size dinÃ¢mico
batch_hours = Variable.get("batch_hours", default_var=1)

# Alertas por email
email_alerts = Variable.get("email_alerts", default_var=True)
```

### ConexÃµes NecessÃ¡rias
1. **BigQuery**: Configurar credentials do GCP
2. **dbt Profile**: Validar profiles.yml

## ğŸ“ˆ Monitoramento

### MÃ©tricas Principais
- **SLA**: Tasks devem completar em < 30 min
- **Success Rate**: > 95% das execuÃ§Ãµes
- **Data Freshness**: Dados com < 2 horas de latÃªncia

### Alertas
- Email em caso de falha
- Slack para alertas crÃ­ticos
- Retry automÃ¡tico (2x com 5min de intervalo)

## ğŸ› ï¸ Desenvolvimento

### Testando DAGs Localmente
```bash
# Validar sintaxe da DAG
python dags/olist_data_pipeline.py

# Testar task especÃ­fica
airflow tasks test olist_data_pipeline staging_orders 2024-01-01
```

### VariÃ¡veis de Ambiente
```bash
export AIRFLOW_HOME=/path/to/airflow
export DBT_PROJECT_DIR=/path/to/dbt
export DBT_PROFILES_DIR=/path/to/dbt
```

## ğŸ”’ SeguranÃ§a

- Credenciais via Airflow Connections
- Secrets via Environment Variables
- RBAC habilitado para produÃ§Ã£o

## ğŸ“ Logs

Logs estÃ£o disponÃ­veis em:
- **Airflow UI**: http://localhost:8080
- **Sistema**: `airflow/logs/`
- **dbt**: Dentro dos logs das tasks

## ğŸš¨ Troubleshooting

### Problemas Comuns

1. **DAG nÃ£o aparece**:
   - Verificar sintaxe Python
   - Checar logs do scheduler

2. **Falha no dbt**:
   - Validar profiles.yml
   - Verificar credenciais BigQuery

3. **Timeout de tasks**:
   - Aumentar batch_hours
   - Revisar recursos disponÃ­veis

### Comandos Ãšteis
```bash
# Reiniciar scheduler
airflow scheduler --daemon

# Limpar estado de DAG
airflow dags delete olist_data_pipeline

# Ver logs em tempo real
tail -f airflow/logs/scheduler/latest/scheduler.log
```

## ğŸ“ Suporte

Para questÃµes sobre:
- **Airflow**: DocumentaÃ§Ã£o oficial Apache Airflow
- **dbt**: DocumentaÃ§Ã£o oficial dbt
- **BigQuery**: DocumentaÃ§Ã£o Google Cloud Platform

2025-12-13 17:31:39,421 INFO - Loaded executor: SequentialExecutor
2025-12-13 17:31:39,462 INFO - Starting the scheduler
2025-12-13 17:31:39,463 INFO - Processing each file at most -1 times
2025-12-13 17:31:39,469 INFO - Launched DagFileProcessorManager with pid: 72792
2025-12-13 17:31:39,471 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 17:31:39,478 INFO - Configured default timezone America/Sao_Paulo
2025-12-13 17:36:39,534 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 17:38:41,958 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [scheduled]>
2025-12-13 17:38:41,958 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 17:38:41,959 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [scheduled]>
2025-12-13 17:38:41,962 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 17:38:41,962 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='run_staging_models', run_id='manual__2024-01-01T00:00:00-03:00', try_number=2, map_index=-1) to SequentialExecutor with priority 4 and queue default
2025-12-13 17:38:41,962 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'run_staging_models', 'manual__2024-01-01T00:00:00-03:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:38:41,967 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'run_staging_models', 'manual__2024-01-01T00:00:00-03:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:38:44,273 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='run_staging_models', run_id='manual__2024-01-01T00:00:00-03:00', try_number=2, map_index=-1)
2025-12-13 17:38:44,279 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=run_staging_models, run_id=manual__2024-01-01T00:00:00-03:00, map_index=-1, run_start_date=2025-12-13 17:38:43.717546+00:00, run_end_date=2025-12-13 17:38:44.136079+00:00, run_duration=0.418533, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=2, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-12-13 17:38:41.960091+00:00, queued_by_job_id=1, pid=79946
2025-12-13 17:41:39,578 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 17:43:44,849 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [scheduled]>
2025-12-13 17:43:44,849 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 17:43:44,850 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [scheduled]>
2025-12-13 17:43:44,851 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.run_staging_models manual__2024-01-01T00:00:00-03:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 17:43:44,852 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='run_staging_models', run_id='manual__2024-01-01T00:00:00-03:00', try_number=3, map_index=-1) to SequentialExecutor with priority 4 and queue default
2025-12-13 17:43:44,852 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'run_staging_models', 'manual__2024-01-01T00:00:00-03:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:43:44,857 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'run_staging_models', 'manual__2024-01-01T00:00:00-03:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:43:47,145 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='run_staging_models', run_id='manual__2024-01-01T00:00:00-03:00', try_number=3, map_index=-1)
2025-12-13 17:43:47,149 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=run_staging_models, run_id=manual__2024-01-01T00:00:00-03:00, map_index=-1, run_start_date=2025-12-13 17:43:46.598374+00:00, run_end_date=2025-12-13 17:43:46.756596+00:00, run_duration=0.158222, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-12-13 17:43:44.850957+00:00, queued_by_job_id=1, pid=80911
2025-12-13 17:43:47,810 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
2025-12-13 17:43:47,811 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 17:43:47,811 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
2025-12-13 17:43:47,812 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 17:43:47,813 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 17:43:47,813 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:43:47,817 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:43:50,137 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=1, map_index=-1)
2025-12-13 17:43:50,141 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:38:12.108287+00:00, map_index=-1, run_start_date=2025-12-13 17:43:49.598210+00:00, run_end_date=2025-12-13 17:43:49.751262+00:00, run_duration=0.153052, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:43:47.812215+00:00, queued_by_job_id=1, pid=81112
2025-12-13 17:46:39,620 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 17:48:49,990 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
2025-12-13 17:48:49,990 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 17:48:49,990 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
2025-12-13 17:48:49,992 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 17:48:49,992 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 17:48:49,992 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:48:49,996 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:48:52,333 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=2, map_index=-1)
2025-12-13 17:48:52,337 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:38:12.108287+00:00, map_index=-1, run_start_date=2025-12-13 17:48:51.786508+00:00, run_end_date=2025-12-13 17:48:51.934865+00:00, run_duration=0.148357, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=5, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:48:49.991405+00:00, queued_by_job_id=1, pid=83268
2025-12-13 17:51:39,664 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 17:53:52,858 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
2025-12-13 17:53:52,858 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 17:53:52,859 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
2025-12-13 17:53:52,860 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 17:53:52,860 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 17:53:52,861 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:53:52,865 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:53:55,179 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=3, map_index=-1)
2025-12-13 17:53:55,183 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:38:12.108287+00:00, map_index=-1, run_start_date=2025-12-13 17:53:54.614212+00:00, run_end_date=2025-12-13 17:53:54.782413+00:00, run_duration=0.168201, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=2, job_id=6, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:53:52.859707+00:00, queued_by_job_id=1, pid=83309
2025-12-13 17:53:58,101 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 17:38:12.108287+00:00: manual__2025-12-13T17:38:12.108287+00:00, state:running, queued_at: 2025-12-13 17:38:12.124051+00:00. externally triggered: True> failed
2025-12-13 17:53:58,102 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 17:38:12.108287+00:00, run_id=manual__2025-12-13T17:38:12.108287+00:00, run_start_date=2025-12-13 17:43:47.788171+00:00, run_end_date=2025-12-13 17:53:58.102413+00:00, run_duration=610.314242, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 16:38:12.108287+00:00, data_interval_end=2025-12-13 17:38:12.108287+00:00, dag_hash=ddcd72763e6860eb1041df6c52065120
2025-12-13 17:53:59,165 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [scheduled]>
2025-12-13 17:53:59,165 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 17:53:59,165 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [scheduled]>
2025-12-13 17:53:59,167 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 17:53:59,167 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:40:15.725110+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 17:53:59,168 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:40:15.725110+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:53:59,171 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:40:15.725110+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:54:01,462 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:40:15.725110+00:00', try_number=1, map_index=-1)
2025-12-13 17:54:01,466 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:40:15.725110+00:00, map_index=-1, run_start_date=2025-12-13 17:54:00.908124+00:00, run_end_date=2025-12-13 17:54:01.063873+00:00, run_duration=0.155749, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=7, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:53:59.166432+00:00, queued_by_job_id=1, pid=83315
2025-12-13 17:54:48,491 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [scheduled]>
2025-12-13 17:54:48,491 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 17:54:48,491 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [scheduled]>
2025-12-13 17:54:48,493 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:40:15.725110+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 17:54:48,493 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:40:15.725110+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 17:54:48,493 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:40:15.725110+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:54:48,498 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:40:15.725110+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:54:50,869 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:40:15.725110+00:00', try_number=2, map_index=-1)
2025-12-13 17:54:50,873 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:40:15.725110+00:00, map_index=-1, run_start_date=2025-12-13 17:54:50.276180+00:00, run_end_date=2025-12-13 17:54:50.429163+00:00, run_duration=0.152983, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=3, job_id=8, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:54:48.492369+00:00, queued_by_job_id=1, pid=83333
2025-12-13 17:55:19,257 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 17:40:15.725110+00:00: manual__2025-12-13T17:40:15.725110+00:00, state:running, queued_at: 2025-12-13 17:40:15.738680+00:00. externally triggered: True> failed
2025-12-13 17:55:19,258 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 17:40:15.725110+00:00, run_id=manual__2025-12-13T17:40:15.725110+00:00, run_start_date=2025-12-13 17:53:59.141341+00:00, run_end_date=2025-12-13 17:55:19.258348+00:00, run_duration=80.117007, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 16:40:15.725110+00:00, data_interval_end=2025-12-13 17:40:15.725110+00:00, dag_hash=ddcd72763e6860eb1041df6c52065120
2025-12-13 17:55:20,313 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
2025-12-13 17:55:20,313 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 17:55:20,314 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>
2025-12-13 17:55:20,315 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:38:12.108287+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 17:55:20,316 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=4, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 17:55:20,316 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:55:20,320 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:38:12.108287+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:55:22,621 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:38:12.108287+00:00', try_number=4, map_index=-1)
2025-12-13 17:55:22,625 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:38:12.108287+00:00, map_index=-1, run_start_date=2025-12-13 17:55:22.054107+00:00, run_end_date=2025-12-13 17:55:22.208246+00:00, run_duration=0.154139, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=4, max_tries=5, job_id=9, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:55:20.314878+00:00, queued_by_job_id=1, pid=83345
2025-12-13 17:55:53,946 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 17:38:12.108287+00:00: manual__2025-12-13T17:38:12.108287+00:00, state:running, queued_at: 2025-12-13 17:54:09.073687+00:00. externally triggered: True> failed
2025-12-13 17:55:53,947 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 17:38:12.108287+00:00, run_id=manual__2025-12-13T17:38:12.108287+00:00, run_start_date=2025-12-13 17:55:20.290933+00:00, run_end_date=2025-12-13 17:55:53.947256+00:00, run_duration=33.656323, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 16:38:12.108287+00:00, data_interval_end=2025-12-13 17:38:12.108287+00:00, dag_hash=ddcd72763e6860eb1041df6c52065120
2025-12-13 17:55:55,001 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:53:33.631212+00:00 [scheduled]>
2025-12-13 17:55:55,001 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 17:55:55,002 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:53:33.631212+00:00 [scheduled]>
2025-12-13 17:55:55,003 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T17:53:33.631212+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 17:55:55,003 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:53:33.631212+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 17:55:55,004 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:53:33.631212+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:55:55,008 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T17:53:33.631212+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 17:55:57,323 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T17:53:33.631212+00:00', try_number=1, map_index=-1)
2025-12-13 17:55:57,327 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T17:53:33.631212+00:00, map_index=-1, run_start_date=2025-12-13 17:55:56.766156+00:00, run_end_date=2025-12-13 17:55:56.920243+00:00, run_duration=0.154087, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=10, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 17:55:55.002738+00:00, queued_by_job_id=1, pid=83352
2025-12-13 17:56:39,706 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 17:57:08,602 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 17:53:33.631212+00:00: manual__2025-12-13T17:53:33.631212+00:00, state:running, queued_at: 2025-12-13 17:53:33.649016+00:00. externally triggered: True> failed
2025-12-13 17:57:08,602 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 17:53:33.631212+00:00, run_id=manual__2025-12-13T17:53:33.631212+00:00, run_start_date=2025-12-13 17:55:54.979762+00:00, run_end_date=2025-12-13 17:57:08.602846+00:00, run_duration=73.623084, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 16:53:33.631212+00:00, data_interval_end=2025-12-13 17:53:33.631212+00:00, dag_hash=1a042fa87aa613e8093b4a8ac4bedfd9
2025-12-13 18:01:39,723 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 18:01:43,782 INFO - DAG olist_data_pipeline is at (or above) max_active_runs (1 of 1), not creating any more runs
2025-12-13 18:01:43,813 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps scheduled__2025-12-13T17:00:00+00:00 [scheduled]>
2025-12-13 18:01:43,813 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:01:43,814 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps scheduled__2025-12-13T17:00:00+00:00 [scheduled]>
2025-12-13 18:01:43,815 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps scheduled__2025-12-13T17:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:01:43,815 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='scheduled__2025-12-13T17:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:01:43,816 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'scheduled__2025-12-13T17:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:01:43,819 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'scheduled__2025-12-13T17:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:01:46,140 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='scheduled__2025-12-13T17:00:00+00:00', try_number=1, map_index=-1)
2025-12-13 18:01:46,144 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=scheduled__2025-12-13T17:00:00+00:00, map_index=-1, run_start_date=2025-12-13 18:01:45.592745+00:00, run_end_date=2025-12-13 18:01:45.750170+00:00, run_duration=0.157425, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=11, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:01:43.814705+00:00, queued_by_job_id=1, pid=86666
2025-12-13 18:06:18,268 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 17:00:00+00:00: scheduled__2025-12-13T17:00:00+00:00, state:running, queued_at: 2025-12-13 18:01:43.777878+00:00. externally triggered: False> failed
2025-12-13 18:06:18,269 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 17:00:00+00:00, run_id=scheduled__2025-12-13T17:00:00+00:00, run_start_date=2025-12-13 18:01:43.792381+00:00, run_end_date=2025-12-13 18:06:18.269366+00:00, run_duration=274.476985, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-12-13 17:00:00+00:00, data_interval_end=2025-12-13 18:00:00+00:00, dag_hash=507b8780e869bd468d2cca259e6e880b
2025-12-13 18:06:18,271 INFO - DAG olist_data_pipeline is at (or above) max_active_runs (1 of 1), not creating any more runs
2025-12-13 18:06:18,760 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:05:41.863370+00:00 [scheduled]>
2025-12-13 18:06:18,760 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:06:18,761 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:05:41.863370+00:00 [scheduled]>
2025-12-13 18:06:18,762 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:05:41.863370+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:06:18,762 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:05:41.863370+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:06:18,763 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:05:41.863370+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:06:18,766 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:05:41.863370+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:06:21,082 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:05:41.863370+00:00', try_number=1, map_index=-1)
2025-12-13 18:06:21,086 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:05:41.863370+00:00, map_index=-1, run_start_date=2025-12-13 18:06:20.521853+00:00, run_end_date=2025-12-13 18:06:20.676612+00:00, run_duration=0.154759, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=12, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:06:18.761649+00:00, queued_by_job_id=1, pid=88264
2025-12-13 18:06:39,765 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 18:08:27,388 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:05:41.863370+00:00: manual__2025-12-13T18:05:41.863370+00:00, state:running, queued_at: 2025-12-13 18:05:41.883385+00:00. externally triggered: True> failed
2025-12-13 18:08:27,388 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:05:41.863370+00:00, run_id=manual__2025-12-13T18:05:41.863370+00:00, run_start_date=2025-12-13 18:06:18.738855+00:00, run_end_date=2025-12-13 18:08:27.388714+00:00, run_duration=128.649859, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:05:41.863370+00:00, data_interval_end=2025-12-13 18:05:41.863370+00:00, dag_hash=a7520b485e48cae81884a168c46a1b46
2025-12-13 18:08:40,889 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [scheduled]>
2025-12-13 18:08:40,890 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:08:40,890 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [scheduled]>
2025-12-13 18:08:40,892 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:08:40,892 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:08:39.403072+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:08:40,892 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:08:39.403072+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:08:40,896 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:08:39.403072+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:08:43,255 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:08:39.403072+00:00', try_number=1, map_index=-1)
2025-12-13 18:08:43,259 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:08:39.403072+00:00, map_index=-1, run_start_date=2025-12-13 18:08:42.677104+00:00, run_end_date=2025-12-13 18:08:42.834411+00:00, run_duration=0.157307, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=13, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:08:40.891323+00:00, queued_by_job_id=1, pid=88795
2025-12-13 18:11:39,809 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 18:13:43,191 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [scheduled]>
2025-12-13 18:13:43,191 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:13:43,192 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [scheduled]>
2025-12-13 18:13:43,193 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:08:39.403072+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:13:43,194 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:08:39.403072+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:13:43,194 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:08:39.403072+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:13:43,198 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:08:39.403072+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:13:45,539 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:08:39.403072+00:00', try_number=2, map_index=-1)
2025-12-13 18:13:45,543 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:08:39.403072+00:00, map_index=-1, run_start_date=2025-12-13 18:13:44.965791+00:00, run_end_date=2025-12-13 18:13:45.134607+00:00, run_duration=0.168816, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=14, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:13:43.192818+00:00, queued_by_job_id=1, pid=90908
2025-12-13 18:16:39,853 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 18:16:45,655 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:08:39.403072+00:00: manual__2025-12-13T18:08:39.403072+00:00, state:running, queued_at: 2025-12-13 18:08:39.413900+00:00. externally triggered: True> failed
2025-12-13 18:16:45,656 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:08:39.403072+00:00, run_id=manual__2025-12-13T18:08:39.403072+00:00, run_start_date=2025-12-13 18:08:40.868631+00:00, run_end_date=2025-12-13 18:16:45.656343+00:00, run_duration=484.787712, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:08:39.403072+00:00, data_interval_end=2025-12-13 18:08:39.403072+00:00, dag_hash=b84b530224233739795dbca4f33b9e9e
2025-12-13 18:16:50,963 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:16:49.854927+00:00 [scheduled]>
2025-12-13 18:16:50,963 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:16:50,964 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:16:49.854927+00:00 [scheduled]>
2025-12-13 18:16:50,965 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:16:49.854927+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:16:50,966 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:16:49.854927+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:16:50,966 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:16:49.854927+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:16:50,970 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:16:49.854927+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:16:53,270 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:16:49.854927+00:00', try_number=1, map_index=-1)
2025-12-13 18:16:53,274 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:16:49.854927+00:00, map_index=-1, run_start_date=2025-12-13 18:16:52.699970+00:00, run_end_date=2025-12-13 18:16:52.858699+00:00, run_duration=0.158729, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=15, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:16:50.964760+00:00, queued_by_job_id=1, pid=94014
2025-12-13 18:17:27,776 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:16:49.854927+00:00: manual__2025-12-13T18:16:49.854927+00:00, state:running, queued_at: 2025-12-13 18:16:49.866945+00:00. externally triggered: True> failed
2025-12-13 18:17:27,776 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:16:49.854927+00:00, run_id=manual__2025-12-13T18:16:49.854927+00:00, run_start_date=2025-12-13 18:16:50.941287+00:00, run_end_date=2025-12-13 18:17:27.776750+00:00, run_duration=36.835463, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:16:49.854927+00:00, data_interval_end=2025-12-13 18:16:49.854927+00:00, dag_hash=b84b530224233739795dbca4f33b9e9e
2025-12-13 18:17:30,302 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:17:29.776108+00:00 [scheduled]>
2025-12-13 18:17:30,303 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:17:30,303 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:17:29.776108+00:00 [scheduled]>
2025-12-13 18:17:30,304 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:17:29.776108+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:17:30,305 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:17:29.776108+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:17:30,305 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:17:29.776108+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:17:30,309 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:17:29.776108+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:17:32,560 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:17:29.776108+00:00', try_number=1, map_index=-1)
2025-12-13 18:17:32,565 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:17:29.776108+00:00, map_index=-1, run_start_date=2025-12-13 18:17:32.025191+00:00, run_end_date=2025-12-13 18:17:32.178140+00:00, run_duration=0.152949, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=16, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:17:30.303943+00:00, queued_by_job_id=1, pid=94266
2025-12-13 18:21:34,705 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:17:29.776108+00:00: manual__2025-12-13T18:17:29.776108+00:00, state:running, queued_at: 2025-12-13 18:17:29.787331+00:00. externally triggered: True> failed
2025-12-13 18:21:34,705 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:17:29.776108+00:00, run_id=manual__2025-12-13T18:17:29.776108+00:00, run_start_date=2025-12-13 18:17:30.280825+00:00, run_end_date=2025-12-13 18:21:34.705915+00:00, run_duration=244.42509, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:17:29.776108+00:00, data_interval_end=2025-12-13 18:17:29.776108+00:00, dag_hash=85d84bc0e65e1da28f644ab6f93d63f9
2025-12-13 18:21:39,885 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 18:22:06,863 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:22:05.429871+00:00 [scheduled]>
2025-12-13 18:22:06,864 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:22:06,864 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:22:05.429871+00:00 [scheduled]>
2025-12-13 18:22:06,865 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:22:05.429871+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:22:06,866 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:22:05.429871+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:22:06,866 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:22:05.429871+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:22:06,870 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:22:05.429871+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:22:09,145 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:22:05.429871+00:00', try_number=1, map_index=-1)
2025-12-13 18:22:09,149 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:22:05.429871+00:00, map_index=-1, run_start_date=2025-12-13 18:22:08.599748+00:00, run_end_date=2025-12-13 18:22:08.751076+00:00, run_duration=0.151328, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=17, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:22:06.865106+00:00, queued_by_job_id=1, pid=95090
2025-12-13 18:26:22,883 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:22:05.429871+00:00: manual__2025-12-13T18:22:05.429871+00:00, state:running, queued_at: 2025-12-13 18:22:05.443355+00:00. externally triggered: True> failed
2025-12-13 18:26:22,883 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:22:05.429871+00:00, run_id=manual__2025-12-13T18:22:05.429871+00:00, run_start_date=2025-12-13 18:22:06.841950+00:00, run_end_date=2025-12-13 18:26:22.883543+00:00, run_duration=256.041593, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:22:05.429871+00:00, data_interval_end=2025-12-13 18:22:05.429871+00:00, dag_hash=7fd8b63a2bc08b52e367d2be1c072786
2025-12-13 18:26:23,936 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:26:22.441273+00:00 [scheduled]>
2025-12-13 18:26:23,937 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:26:23,937 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:26:22.441273+00:00 [scheduled]>
2025-12-13 18:26:23,938 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:26:22.441273+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:26:23,939 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:26:22.441273+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:26:23,939 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:26:22.441273+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:26:23,943 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:26:22.441273+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:26:26,317 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:26:22.441273+00:00', try_number=1, map_index=-1)
2025-12-13 18:26:26,321 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:26:22.441273+00:00, map_index=-1, run_start_date=2025-12-13 18:26:25.759540+00:00, run_end_date=2025-12-13 18:26:25.920407+00:00, run_duration=0.160867, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=18, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:26:23.938087+00:00, queued_by_job_id=1, pid=99272
2025-12-13 18:26:39,929 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 18:28:41,325 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:26:22.441273+00:00: manual__2025-12-13T18:26:22.441273+00:00, state:running, queued_at: 2025-12-13 18:26:22.453066+00:00. externally triggered: True> failed
2025-12-13 18:28:41,325 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:26:22.441273+00:00, run_id=manual__2025-12-13T18:26:22.441273+00:00, run_start_date=2025-12-13 18:26:23.915648+00:00, run_end_date=2025-12-13 18:28:41.325885+00:00, run_duration=137.410237, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:26:22.441273+00:00, data_interval_end=2025-12-13 18:26:22.441273+00:00, dag_hash=7fd8b63a2bc08b52e367d2be1c072786
2025-12-13 18:28:48,302 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:28:47.475210+00:00 [scheduled]>
2025-12-13 18:28:48,303 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:28:48,303 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:28:47.475210+00:00 [scheduled]>
2025-12-13 18:28:48,304 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:28:47.475210+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:28:48,305 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:28:47.475210+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:28:48,305 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:28:47.475210+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:28:48,309 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:28:47.475210+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:28:50,591 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:28:47.475210+00:00', try_number=1, map_index=-1)
2025-12-13 18:28:50,595 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:28:47.475210+00:00, map_index=-1, run_start_date=2025-12-13 18:28:50.050049+00:00, run_end_date=2025-12-13 18:28:50.209458+00:00, run_duration=0.159409, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=19, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:28:48.303917+00:00, queued_by_job_id=1, pid=99896
2025-12-13 18:29:48,519 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:28:47.475210+00:00: manual__2025-12-13T18:28:47.475210+00:00, state:running, queued_at: 2025-12-13 18:28:47.482929+00:00. externally triggered: True> failed
2025-12-13 18:29:48,520 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:28:47.475210+00:00, run_id=manual__2025-12-13T18:28:47.475210+00:00, run_start_date=2025-12-13 18:28:48.281282+00:00, run_end_date=2025-12-13 18:29:48.519982+00:00, run_duration=60.2387, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:28:47.475210+00:00, data_interval_end=2025-12-13 18:28:47.475210+00:00, dag_hash=7fd8b63a2bc08b52e367d2be1c072786
2025-12-13 18:31:39,961 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 18:33:50,318 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [scheduled]>
2025-12-13 18:33:50,319 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:33:50,319 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [scheduled]>
2025-12-13 18:33:50,320 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:33:50,321 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:33:49.127472+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:33:50,321 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:33:49.127472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:33:50,325 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:33:49.127472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:33:52,614 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:33:49.127472+00:00', try_number=1, map_index=-1)
2025-12-13 18:33:52,618 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:33:49.127472+00:00, map_index=-1, run_start_date=2025-12-13 18:33:52.066447+00:00, run_end_date=2025-12-13 18:33:52.221421+00:00, run_duration=0.154974, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=20, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:33:50.319901+00:00, queued_by_job_id=1, pid=101570
2025-12-13 18:36:40,003 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 18:38:53,116 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [scheduled]>
2025-12-13 18:38:53,117 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:38:53,117 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [scheduled]>
2025-12-13 18:38:53,119 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:33:49.127472+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:38:53,119 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:33:49.127472+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:38:53,120 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:33:49.127472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:38:53,124 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:33:49.127472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:38:55,427 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:33:49.127472+00:00', try_number=2, map_index=-1)
2025-12-13 18:38:55,431 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:33:49.127472+00:00, map_index=-1, run_start_date=2025-12-13 18:38:54.868050+00:00, run_end_date=2025-12-13 18:38:55.018490+00:00, run_duration=0.15044, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=21, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:38:53.118257+00:00, queued_by_job_id=1, pid=103443
2025-12-13 18:39:42,640 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:33:49.127472+00:00: manual__2025-12-13T18:33:49.127472+00:00, state:running, queued_at: 2025-12-13 18:33:49.138998+00:00. externally triggered: True> failed
2025-12-13 18:39:42,641 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:33:49.127472+00:00, run_id=manual__2025-12-13T18:33:49.127472+00:00, run_start_date=2025-12-13 18:33:50.297910+00:00, run_end_date=2025-12-13 18:39:42.641488+00:00, run_duration=352.343578, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:33:49.127472+00:00, data_interval_end=2025-12-13 18:33:49.127472+00:00, dag_hash=c0bd5b3bcf52c5ee975bb131e83a1b14
2025-12-13 18:39:45,604 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:39:44.313347+00:00 [scheduled]>
2025-12-13 18:39:45,604 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:39:45,605 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:39:44.313347+00:00 [scheduled]>
2025-12-13 18:39:45,606 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:39:44.313347+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:39:45,606 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:39:44.313347+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:39:45,607 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:39:44.313347+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:39:45,611 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:39:44.313347+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:39:47,915 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:39:44.313347+00:00', try_number=1, map_index=-1)
2025-12-13 18:39:47,919 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:39:44.313347+00:00, map_index=-1, run_start_date=2025-12-13 18:39:47.364801+00:00, run_end_date=2025-12-13 18:39:47.522121+00:00, run_duration=0.15732, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=22, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:39:45.605699+00:00, queued_by_job_id=1, pid=103683
2025-12-13 18:41:40,047 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 18:41:56,951 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:39:44.313347+00:00: manual__2025-12-13T18:39:44.313347+00:00, state:running, queued_at: 2025-12-13 18:39:44.324888+00:00. externally triggered: True> failed
2025-12-13 18:41:56,952 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:39:44.313347+00:00, run_id=manual__2025-12-13T18:39:44.313347+00:00, run_start_date=2025-12-13 18:39:45.581550+00:00, run_end_date=2025-12-13 18:41:56.952163+00:00, run_duration=131.370613, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:39:44.313347+00:00, data_interval_end=2025-12-13 18:39:44.313347+00:00, dag_hash=133c01d83d42e0b19420ba1a11a243ae
2025-12-13 18:43:22,204 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:43:21.275679+00:00 [scheduled]>
2025-12-13 18:43:22,204 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:43:22,205 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:43:21.275679+00:00 [scheduled]>
2025-12-13 18:43:22,206 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:43:21.275679+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:43:22,206 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:43:21.275679+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:43:22,207 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:43:21.275679+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:43:22,211 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:43:21.275679+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:43:24,507 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:43:21.275679+00:00', try_number=1, map_index=-1)
2025-12-13 18:43:24,511 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:43:21.275679+00:00, map_index=-1, run_start_date=2025-12-13 18:43:23.961013+00:00, run_end_date=2025-12-13 18:43:24.119234+00:00, run_duration=0.158221, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=23, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:43:22.205685+00:00, queued_by_job_id=1, pid=105261
2025-12-13 18:45:20,953 ERROR - Marking run <DagRun olist_data_pipeline @ 2025-12-13 18:43:21.275679+00:00: manual__2025-12-13T18:43:21.275679+00:00, state:running, queued_at: 2025-12-13 18:43:21.283419+00:00. externally triggered: True> failed
2025-12-13 18:45:20,954 INFO - DagRun Finished: dag_id=olist_data_pipeline, execution_date=2025-12-13 18:43:21.275679+00:00, run_id=manual__2025-12-13T18:43:21.275679+00:00, run_start_date=2025-12-13 18:43:22.182397+00:00, run_end_date=2025-12-13 18:45:20.954265+00:00, run_duration=118.771868, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-12-13 17:43:21.275679+00:00, data_interval_end=2025-12-13 18:43:21.275679+00:00, dag_hash=2fce28ae88e791965b4f4102d100be12
2025-12-13 18:45:26,450 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [scheduled]>
2025-12-13 18:45:26,451 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:45:26,451 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [scheduled]>
2025-12-13 18:45:26,452 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:45:26,453 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:45:25.229472+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:45:26,453 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:45:25.229472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:45:26,457 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:45:25.229472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:45:28,815 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:45:25.229472+00:00', try_number=1, map_index=-1)
2025-12-13 18:45:28,819 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:45:25.229472+00:00, map_index=-1, run_start_date=2025-12-13 18:45:28.224853+00:00, run_end_date=2025-12-13 18:45:28.384720+00:00, run_duration=0.159867, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=24, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:45:26.451944+00:00, queued_by_job_id=1, pid=105935
2025-12-13 18:46:40,089 INFO - Adopting or resetting orphaned tasks for active dag runs
2025-12-13 18:50:28,716 INFO - 1 tasks up for execution:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [scheduled]>
2025-12-13 18:50:28,716 INFO - DAG olist_data_pipeline has 0/16 running and queued tasks
2025-12-13 18:50:28,717 INFO - Setting the following tasks to queued state:
	<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [scheduled]>
2025-12-13 18:50:28,718 INFO - Trying to enqueue tasks: [<TaskInstance: olist_data_pipeline.dbt_deps manual__2025-12-13T18:45:25.229472+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2025-12-13 18:50:28,719 INFO - Sending TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:45:25.229472+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 5 and queue default
2025-12-13 18:50:28,719 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:45:25.229472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:50:28,723 INFO - Executing command: ['airflow', 'tasks', 'run', 'olist_data_pipeline', 'dbt_deps', 'manual__2025-12-13T18:45:25.229472+00:00', '--local', '--subdir', 'DAGS_FOLDER/olist_data_pipeline.py']
2025-12-13 18:50:31,036 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='olist_data_pipeline', task_id='dbt_deps', run_id='manual__2025-12-13T18:45:25.229472+00:00', try_number=2, map_index=-1)
2025-12-13 18:50:31,040 INFO - TaskInstance Finished: dag_id=olist_data_pipeline, task_id=dbt_deps, run_id=manual__2025-12-13T18:45:25.229472+00:00, map_index=-1, run_start_date=2025-12-13 18:50:30.492535+00:00, run_end_date=2025-12-13 18:50:30.644272+00:00, run_duration=0.151737, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=25, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-12-13 18:50:28.717760+00:00, queued_by_job_id=1, pid=106800
